{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shapely import wkt, geometry\n",
    "from shapely.wkt import loads, dumps\n",
    "from shapely.ops import unary_union, polygonize\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, Point, LineString, MultiPolygon\n",
    "import math\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "from rtree import index\n",
    "from pydrive2.drive import GoogleDrive\n",
    "from pydrive2.auth import GoogleAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace this with the path to the shapefile\n",
    "gadm=r\"C:\\Users\\user\\Documents\\Python\\PythonNotebook\\updated_uganda_shapefile\\updated_uganda_shapefile.shp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate to get the file ID of the global oevrlap shapefile\n",
    "#Run this cell only once!!!. Comment out when not in use to reduce the instances of google auth requests which are limited.\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "drive = GoogleDrive(gauth)\n",
    "filename = 'Shapefile_for_overlap_check.xlsx'\n",
    "query = f\"title contains '{filename}' and trashed = false\"\n",
    "file_list = drive.ListFile({'q': query}).GetList()\n",
    "# Print file IDs of matching files\n",
    "for file in file_list:\n",
    "    print(f\"File Name: {file['title']}, File ID: {file['id']},Date: {file['modifiedDate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download global overlap file using the latest id and write to CSV\n",
    "#This too should be done once to save on time.\n",
    "file_id = '1OUlENlmX5GKZk6KBRZW64R_HGWuihbcQ'  #Replace with the latest file Id from the above process\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "downloaded.GetContentFile('Shapefile.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Read either Excel (.xlsx, .xls) or CSV (.csv) files\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the file\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The data from the file\n",
    "    \"\"\"\n",
    "    # Get the file extension\n",
    "    _, file_extension = os.path.splitext(file_path.lower())\n",
    "    \n",
    "    try:\n",
    "        # Read Excel files\n",
    "        if file_extension in ['.xlsx', '.xls']:\n",
    "            return pd.read_excel(file_path)\n",
    "        # Read CSV files\n",
    "        elif file_extension == '.csv':\n",
    "            return pd.read_csv(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_extension}. Please use Excel (.xlsx, .xls) or CSV (.csv) files.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input files\n",
    "print(\"Reading input files...\")\n",
    "data = read_file(r\"C:\\Users\\user\\Documents\\Xavier Polygons 2025\\January\\31st\\1738305868978_cce-polygon-drawings_16072_20250131_064428.xlsx\")\n",
    "global_overlap_df = pd.read_excel('Shapefile.xlsx')\n",
    "\n",
    "# Rename data columns based on how they were configured\n",
    "df=data.rename(columns={'Database ID':'id','Polygon ID':'polygon_id',\"Pula Box ID\":\"boxes_pula_id\",\"q_farm_polygon_drawing\":\"Farm_polygon\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert WKT column to geometry\n",
    "global_overlap_df['geometry'] = gpd.GeoSeries.from_wkt(global_overlap_df.geometry)\n",
    "global_overlap_df = gpd.GeoDataFrame(global_overlap_df, crs=\"EPSG:4326\", geometry=\"geometry\")\n",
    "global_overlap_df=global_overlap_df.drop('polygon_id', axis=1)\n",
    "global_overlap_df=global_overlap_df.rename(columns={'db_polygon_id':'polygon_id'})\n",
    "print(f\"Global Overlap df size: {len(global_overlap_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def literal_return(val):\n",
    "    \"\"\"\n",
    "    Safely evaluate string representation of Python literals\n",
    "    Returns the original value if evaluation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(val)  \n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, identify rows with NAs in Farm_polygon\n",
    "na_rows = df[df['Farm_polygon'].isna()].copy()\n",
    "na_rows['Polygon Status'] = 'redo'\n",
    "\n",
    "# Initialize a DataFrame to store all invalid geometries\n",
    "invalid_rows = pd.DataFrame()\n",
    "\n",
    "# Process non-NA rows\n",
    "df_valid = df.dropna(subset=[\"Farm_polygon\"]).copy()\n",
    "\n",
    "valid_geom_rows = []  # List to store rows with valid geometries\n",
    "geom = []  # List to store the valid geometries\n",
    "lent = []  # To track lengths of the polygons\n",
    "\n",
    "for i, row in enumerate(df_valid.itertuples(), 1):\n",
    "    poly = literal_return(row.Farm_polygon)\n",
    "    \n",
    "    # Ensure poly is a list of dictionaries\n",
    "    if isinstance(poly, list) and all(isinstance(p, dict) for p in poly):\n",
    "        lent.append(len(poly))\n",
    "        rw = []\n",
    "\n",
    "        # Extract longitude and latitude\n",
    "        for j in range(len(poly)):\n",
    "            if 'longitude' in poly[j] and 'latitude' in poly[j]:\n",
    "                holder = poly[j]['longitude'], poly[j]['latitude']\n",
    "                rw.append(holder)\n",
    "            else:\n",
    "                print(f\"Row {i} contains a point without longitude/latitude: {poly[j]}\")\n",
    "                # Add to invalid rows\n",
    "                invalid_row = df_valid.iloc[row.Index:row.Index+1].copy()\n",
    "                invalid_row['Polygon Status'] = 'redo'\n",
    "                invalid_rows = pd.concat([invalid_rows, invalid_row])\n",
    "                continue\n",
    "\n",
    "        if len(rw) == 1:  # Invalid polygon with single point\n",
    "            invalid_row = df_valid.iloc[row.Index:row.Index+1].copy()\n",
    "            invalid_row['Polygon Status'] = 'redo'\n",
    "            invalid_rows = pd.concat([invalid_rows, invalid_row])\n",
    "            continue\n",
    "\n",
    "        if len(rw) > 2:  # Valid polygon\n",
    "            rw.append(rw[0])  # Close the polygon\n",
    "            try:\n",
    "                rw = geometry.Polygon(rw)\n",
    "                if rw.is_valid:\n",
    "                    geom.append(rw)\n",
    "                    valid_geom_rows.append(row.Index)\n",
    "                else:\n",
    "                    invalid_row = df_valid.iloc[row.Index:row.Index+1].copy()\n",
    "                    invalid_row['Polygon Status'] = 'redo'\n",
    "                    invalid_rows = pd.concat([invalid_rows, invalid_row])\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating polygon for row {i}: {e}\")\n",
    "                invalid_row = df_valid.iloc[row.Index:row.Index+1].copy()\n",
    "                invalid_row['Polygon Status'] = 'redo'\n",
    "                invalid_rows = pd.concat([invalid_rows, invalid_row])\n",
    "        else:\n",
    "            print(f\"Row {i} resulted in an invalid polygon (fewer than 3 points).\")\n",
    "            invalid_row = df_valid.iloc[row.Index:row.Index+1].copy()\n",
    "            invalid_row['Polygon Status'] = 'redo'\n",
    "            invalid_rows = pd.concat([invalid_rows, invalid_row])\n",
    "    else:\n",
    "        print(f\"Row {i} has invalid polygon data: {poly}\")\n",
    "        invalid_row = df_valid.iloc[row.Index:row.Index+1].copy()\n",
    "        invalid_row['Polygon Status'] = 'redo'\n",
    "        invalid_rows = pd.concat([invalid_rows, invalid_row])\n",
    "\n",
    "# Combine all invalid rows (NAs and invalid geometries)\n",
    "all_invalid_rows = pd.concat([na_rows, invalid_rows])\n",
    "\n",
    "# Rename columns\n",
    "all_invalid_rows=all_invalid_rows.rename(columns={'id':'Database ID','polygon_id':'Polygon ID',\"boxes_pula_id\":\"Pula Box ID\"})\n",
    "\n",
    "# Save invalid rows to Excel\n",
    "all_invalid_rows.to_csv('Files_to_redo.csv', index=False)\n",
    "\n",
    "# Create final DataFrame with only valid rows\n",
    "df = df_valid.loc[valid_geom_rows].copy()\n",
    "df['geom'] = geom\n",
    "gdf = gpd.GeoDataFrame(df, crs=\"EPSG:4326\", geometry=geom)\n",
    "gdf.drop(\"geom\", axis=1, inplace=True)\n",
    "print(f\"DataFrame size: {len(gdf)}\")\n",
    "print(f\"Number of rows to redo: {len(all_invalid_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shapely## Add Location columns\n",
    "intersection_df = gdf.copy()\n",
    "# intersection_df = df.copy()\n",
    "intersection_df[\"centroid\"] = intersection_df[\"geometry\"].centroid\n",
    "# intersection_df[\"centroid\"] = df[\"validated_polygon_location_centroid_coordinates\"].apply(lambda x: shapely.wkt.loads(x))\n",
    "\n",
    "intersection_df=intersection_df.to_crs('epsg:4326')\n",
    "admin_shapefile = gpd.read_file(gadm)\n",
    "admin_shapefile=admin_shapefile.to_crs('epsg:4326')\n",
    "intersection_df = gpd.GeoDataFrame(intersection_df,crs=admin_shapefile.crs,geometry=\"centroid\")\n",
    "admin_shapefile.columns=admin_shapefile.columns.str.lower()\n",
    "df_points_intersection = sjoin(intersection_df,admin_shapefile,how='left',predicate='intersects')\n",
    "df_points_intersection = df_points_intersection.drop(columns = ['index_right'])\n",
    "df_intersection=df_points_intersection.rename(columns={'name_1':'District','name_4':'Parish','name_3':'Subcounty','name_2':'County'})\n",
    "location_cols = ['polygon_id','District','County',\"Subcounty\",'Parish']\n",
    "location_details=df_intersection[location_cols]\n",
    "location_details=location_details.drop_duplicates(subset='polygon_id')\n",
    "# data2=pd.merge(data,location_details,on=[\"polygon_id\"],how=\"left\")\n",
    "gdf2=pd.merge(gdf,location_details,on=[\"polygon_id\"],how=\"left\")\n",
    "print(\"Data size: \",len(gdf2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Overlapping Shapefile\n",
    "new_df_ids = gdf2['polygon_id']\n",
    "gdf2 = gdf2.to_crs('epsg:4326')\n",
    "\n",
    "global_overlap_df= global_overlap_df[~global_overlap_df['polygon_id'].isin(new_df_ids)]\n",
    "global_overlap_df=global_overlap_df[global_overlap_df['Parish'].isin(gdf2[\"Parish\"])]\n",
    "print(f\"Global overlaping shapefile size: {len(global_overlap_df)}\")\n",
    "#Write the global overlap file to csv\n",
    "# global_overlap_df.to_csv('Shapefile_for_overlap_check.csv', index=False)\n",
    "polygon_df2=global_overlap_df[([\"polygon_id\", \"geometry\"])]\n",
    "polygon_df2 = polygon_df2.rename_geometry('geometry_global_overlap_data')\n",
    "intersection_df = gpd.sjoin(gdf2, global_overlap_df, how='inner', predicate='intersects')\n",
    "intersection_df = intersection_df.rename_geometry('geometry_data')\n",
    "intersection_df=pd.merge(intersection_df,polygon_df2,right_on=\"polygon_id\",left_on=\"polygon_id_right\",how=\"left\")\n",
    "intersection_df=intersection_df.drop('polygon_id', axis=1)\n",
    "intersection_df=intersection_df.rename(columns={'polygon_id_left':'polygon_id'})\n",
    "# intersection_df.to_csv(newpath+\"Global_overlap_data.csv\",index=False)\n",
    "print(f\"Polygons with Global overlaps: {len(intersection_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate polygons\n",
    "# Count the number of duplicates\n",
    "duplicate_count = gdf2.duplicated(subset=['polygon_id'], keep='first').sum()\n",
    "\n",
    "# Print the number of duplicates\n",
    "print(f\"Number of duplicates found: {duplicate_count}\")\n",
    "\n",
    "# Remove duplicates\n",
    "gdf2['is_duplicate'] = gdf2.duplicated(subset=['polygon_id'], keep='first')\n",
    "gdf2 = gdf2.drop_duplicates(subset=['polygon_id'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle triangular polygons\n",
    "def identify_and_simplify_triangles(gdf2, offset_percentage=0.32, iterations=1):\n",
    "\n",
    "    # Identifies triangular polygons and applies QGIS-style simplification that converts\n",
    "    # triangle vertices into sides of a new 6-sided polygon, maintaining shared sides.\n",
    "    \n",
    "    def is_triangle(polygon):\n",
    "        return len(set(polygon.exterior.coords[:-1])) == 3\n",
    "\n",
    "    def qgis_style_simplify(geometry, offset_percentage):\n",
    "        if not is_triangle(geometry):\n",
    "            return geometry\n",
    "        \n",
    "        # Get original vertices (excluding closing point)\n",
    "        vertices = list(geometry.exterior.coords)[:-1]\n",
    "        \n",
    "        # Function to get interpolated points along original sides\n",
    "        def get_side_points(p1, p2, ratio):\n",
    "            # Get point at ratio distance from p1 to p2\n",
    "            x = p1[0] + (p2[0] - p1[0]) * ratio\n",
    "            y = p1[1] + (p2[1] - p1[1]) * ratio\n",
    "            # Get point at same ratio from p2 to p1\n",
    "            x2 = p2[0] + (p1[0] - p2[0]) * ratio\n",
    "            y2 = p2[1] + (p1[1] - p2[1]) * ratio\n",
    "            return (x, y), (x2, y2)\n",
    "\n",
    "        # Generate new vertices for 6-sided polygon\n",
    "        new_vertices = []\n",
    "        for i in range(3):\n",
    "            # Get current and next vertex\n",
    "            p1 = vertices[i]\n",
    "            p2 = vertices[(i + 1) % 3]\n",
    "            \n",
    "            # Get two points along this side\n",
    "            point1, point2 = get_side_points(p1, p2, offset_percentage)\n",
    "            new_vertices.extend([point1, point2])\n",
    "\n",
    "        # Create new polygon\n",
    "        new_polygon = Polygon(new_vertices)\n",
    "        \n",
    "        # Ensure the new polygon is valid\n",
    "        if not new_polygon.is_valid:\n",
    "            new_polygon = new_polygon.buffer(0)\n",
    "        \n",
    "        return new_polygon\n",
    "\n",
    "    # Create a copy of the input GeoDataFrame\n",
    "    result_gdf = gdf2.copy()\n",
    "    \n",
    "    # Filter triangles and apply simplification directly\n",
    "    mask = result_gdf.geometry.apply(is_triangle)\n",
    "    triangle_count = mask.sum()  # Count the number of triangles\n",
    "    result_gdf.loc[mask, 'geometry'] = result_gdf[mask].geometry.apply(\n",
    "        lambda x: qgis_style_simplify(x, offset_percentage)\n",
    "    )\n",
    "    \n",
    "    corrected_count = triangle_count  # All triangles found are corrected\n",
    "    \n",
    "    # Print out the number of triangles found and corrected\n",
    "    print(f\"Number of triangles found: {triangle_count}\")\n",
    "    print(f\"Number of triangles corrected: {corrected_count}\")\n",
    "    \n",
    "    return result_gdf\n",
    "\n",
    "# Process your existing gdf2\n",
    "gdf2 = identify_and_simplify_triangles(gdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tolerance of 2 meters\n",
    "tolerance=2\n",
    "\n",
    "# Reproject to a metric CRS (e.g., UTM zone 33N or EPSG:3857)\n",
    "gdf2 = gdf2.to_crs(epsg=3857)  # Preserves all columns\n",
    "\n",
    "# Remove spikes from polygons\n",
    "gdf2[\"geometry\"] = gdf2[\"geometry\"].buffer(-tolerance).buffer(tolerance)\n",
    "\n",
    "# Remove Duplicate vertices with tolerance\n",
    "gdf2[\"geometry\"] = gdf2.remove_repeated_points(tolerance).geometry\n",
    "\n",
    "# Convert back to WGS84\n",
    "gdf2 = gdf2.to_crs(epsg=4326)  # All columns are still retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert geometry column to WKT strings\n",
    "global_overlap_df['geometry'] = global_overlap_df['geometry'].apply(lambda x: x.wkt)\n",
    "gdf2['geometry'] = gdf2['geometry'].apply(lambda x: x.wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate vertices\n",
    "def remove_duplicate_vertices(geometry_str):\n",
    "    polygon = loads(geometry_str)\n",
    "    coords = list(polygon.exterior.coords)\n",
    "    # Remove duplicate consecutive points\n",
    "    unique_coords = []\n",
    "    for i in range(len(coords)):\n",
    "        if i == 0 or coords[i] != coords[i-1]:\n",
    "            unique_coords.append(coords[i])\n",
    "    # Ensure the polygon is closed\n",
    "    if unique_coords[0] != unique_coords[-1]:\n",
    "        unique_coords.append(unique_coords[0])\n",
    "    return Polygon(unique_coords)\n",
    "\n",
    "gdf2['geometry'] = gdf2['geometry'].apply(lambda x: dumps(remove_duplicate_vertices(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculation of the area in Acres\n",
    "def calculate_area_acres(geometry_str):\n",
    "    polygon = loads(geometry_str)\n",
    "    return polygon.area * 111319.9 * 111319.9 * np.cos(np.radians(polygon.centroid.y)) * 0.000247105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate areas and identify small polygons\n",
    "gdf2['area_acres'] = gdf2['geometry'].apply(calculate_area_acres)\n",
    "gdf2['is_small_polygon'] = gdf2['area_acres'] < 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify self intersections\n",
    "def is_self_intersecting(geometry_str):\n",
    "    polygon = loads(geometry_str)\n",
    "    return not polygon.is_simple\n",
    "\n",
    "# Check for self-intersections\n",
    "gdf2['is_self_intersecting'] = gdf2['geometry'].apply(is_self_intersecting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix self-intersecting polygons\n",
    "def fix_self_intersection(geometry_str):\n",
    "    polygon = loads(geometry_str)\n",
    "    if not polygon.is_simple:\n",
    "        boundary = LineString(polygon.exterior.coords)\n",
    "        valid_polygons = list(polygonize([boundary]))\n",
    "        \n",
    "        if valid_polygons:\n",
    "            largest_polygon = max(valid_polygons, key=lambda p: p.area)\n",
    "            return largest_polygon\n",
    "        else:\n",
    "            return polygon.convex_hull\n",
    "    return polygon\n",
    "\n",
    "print(\"Fixing self-intersecting polygons...\")\n",
    "gdf2['geometry'] = gdf2.apply(\n",
    "    lambda row: dumps(fix_self_intersection(row['geometry'])) if row['is_self_intersecting'] else row['geometry'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reference and input polygon lists\n",
    "print(\"Creating polygon lists and spatial indices...\")\n",
    "reference_polygons = [loads(geom) for geom in global_overlap_df['geometry']]\n",
    "input_polygons = [loads(geom) for geom in gdf2['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spatial indices\n",
    "def create_spatial_index(polygons):\n",
    "    idx = index.Index()\n",
    "    for pos, poly in enumerate(polygons):\n",
    "        idx.insert(pos, poly.bounds)\n",
    "    return idx\n",
    "\n",
    "reference_idx = create_spatial_index(reference_polygons)\n",
    "input_idx = create_spatial_index(input_polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "# 1. Function to check for intersection\n",
    "def check_intersection_with_index(polygon, spatial_index, all_polygons):\n",
    "    bounds = polygon.bounds\n",
    "    potential_matches_idx = list(spatial_index.intersection(bounds))\n",
    "    return any(polygon.intersects(all_polygons[idx]) for idx in potential_matches_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Function to rotate polygon\n",
    "def rotate_polygon(polygon, angle_degrees, origin=None):\n",
    "    \"\"\"Rotate polygon by angle (in degrees) around origin (defaults to centroid).\"\"\"\n",
    "    if origin is None:\n",
    "        origin = polygon.centroid\n",
    "    \n",
    "    angle_radians = math.radians(angle_degrees)\n",
    "    cos_angle = math.cos(angle_radians)\n",
    "    sin_angle = math.sin(angle_radians)\n",
    "    \n",
    "    coords = list(polygon.exterior.coords)\n",
    "    rotated_coords = []\n",
    "    \n",
    "    for x, y in coords:\n",
    "        # Translate to origin\n",
    "        dx = x - origin.x\n",
    "        dy = y - origin.y\n",
    "        \n",
    "        # Rotate\n",
    "        rotated_x = dx * cos_angle - dy * sin_angle\n",
    "        rotated_y = dx * sin_angle + dy * cos_angle\n",
    "        \n",
    "        # Translate back\n",
    "        final_x = rotated_x + origin.x\n",
    "        final_y = rotated_y + origin.y\n",
    "        \n",
    "        rotated_coords.append((final_x, final_y))\n",
    "    \n",
    "    return Polygon(rotated_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Function to find non-intersecting position\n",
    "def find_non_intersecting_position(polygon, reference_idx, reference_polygons, input_idx, input_polygons, current_idx):\n",
    "    \"\"\"Find a non-intersecting position using both translation and rotation.\"\"\"\n",
    "    max_distance = 0.002  # Maximum translation distance\n",
    "    distance_steps = 8    # Number of steps for translation\n",
    "    angle_steps = 36      # Number of angles to try for rotation\n",
    "    max_angle = 360      # Maximum rotation angle\n",
    "    \n",
    "    # Original centroid for reference\n",
    "    original_centroid = polygon.centroid\n",
    "    \n",
    "    # Directions for translation\n",
    "    directions = [(math.cos(2*math.pi*i/8), math.sin(2*math.pi*i/8)) for i in range(8)]\n",
    "    \n",
    "    # Try different combinations of translation and rotation\n",
    "    for distance in np.linspace(0, max_distance, distance_steps):\n",
    "        for dx, dy in directions:\n",
    "            # Calculate translation vector\n",
    "            move_vector = (dx * distance, dy * distance)\n",
    "            \n",
    "            # Try different rotation angles\n",
    "            for angle in np.linspace(0, max_angle, angle_steps):\n",
    "                # First translate\n",
    "                translated_coords = [(x + move_vector[0], y + move_vector[1]) \n",
    "                                  for x, y in polygon.exterior.coords]\n",
    "                translated_polygon = Polygon(translated_coords)\n",
    "                \n",
    "                # Then rotate\n",
    "                rotated_polygon = rotate_polygon(translated_polygon, angle)\n",
    "                \n",
    "                # Check if this position works\n",
    "                if not (check_intersection_with_index(rotated_polygon, reference_idx, reference_polygons) or\n",
    "                       any(rotated_polygon.intersects(other) for i, other in enumerate(input_polygons) \n",
    "                           if i != current_idx)):\n",
    "                    return rotated_polygon, True\n",
    "    \n",
    "    # If no valid position found, try more aggressive adjustments\n",
    "    for distance in np.linspace(max_distance, max_distance * 2, distance_steps):\n",
    "        for dx, dy in directions:\n",
    "            move_vector = (dx * distance, dy * distance)\n",
    "            for angle in np.linspace(0, max_angle, angle_steps):\n",
    "                translated_coords = [(x + move_vector[0], y + move_vector[1]) \n",
    "                                  for x, y in polygon.exterior.coords]\n",
    "                translated_polygon = Polygon(translated_coords)\n",
    "                rotated_polygon = rotate_polygon(translated_polygon, angle)\n",
    "                \n",
    "                if not (check_intersection_with_index(rotated_polygon, reference_idx, reference_polygons) or\n",
    "                       any(rotated_polygon.intersects(other) for i, other in enumerate(input_polygons) \n",
    "                           if i != current_idx)):\n",
    "                    return rotated_polygon, True\n",
    "    \n",
    "    return polygon, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_any_intersection(polygon, reference_polygons, input_polygons, current_idx):\n",
    "    \"\"\"Check if polygon intersects with any reference or input polygon (except itself).\"\"\"\n",
    "    # Check against reference polygons\n",
    "    if any(polygon.intersects(ref_poly) for ref_poly in reference_polygons):\n",
    "        return True\n",
    "    \n",
    "    # Check against other input polygons\n",
    "    return any(polygon.intersects(other) for i, other in enumerate(input_polygons) \n",
    "              if i != current_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process polygons in batches\n",
    "print(\"Processing polygons...\")\n",
    "batch_size = 1000\n",
    "total_rows = len(gdf2)\n",
    "intersection_count = 0\n",
    "fixed_count = 0\n",
    "unfixed_indices = []\n",
    "scaling_count = 0\n",
    "\n",
    "for start_idx in range(0, total_rows, batch_size):\n",
    "    end_idx = min(start_idx + batch_size, total_rows)\n",
    "    batch_indices = gdf2.index[start_idx:end_idx]\n",
    "    \n",
    "    for idx in batch_indices:\n",
    "        row_position = gdf2.index.get_loc(idx)\n",
    "        current_polygon = loads(gdf2.at[idx, 'geometry'])\n",
    "        \n",
    "        # Scale up if too small\n",
    "        if gdf2.at[idx, 'is_small_polygon']:\n",
    "            scaling_count += 1\n",
    "            current_area = gdf2.at[idx, 'area_acres']\n",
    "            target_area = 0.250000000000  # Target area in acres\n",
    "            scale_factor = math.sqrt(target_area / current_area)\n",
    "            \n",
    "            centroid = current_polygon.centroid\n",
    "            scaled_coords = [(centroid.x + (x - centroid.x) * scale_factor,\n",
    "                            centroid.y + (y - centroid.y) * scale_factor)\n",
    "                           for x, y in current_polygon.exterior.coords]\n",
    "            current_polygon = Polygon(scaled_coords)\n",
    "            \n",
    "            # Update the area after scaling\n",
    "            gdf2.at[idx, 'geometry'] = dumps(current_polygon)\n",
    "            gdf2.at[idx, 'area_acres'] = calculate_area_acres(dumps(current_polygon))\n",
    "            gdf2.at[idx, 'is_small_polygon'] = False\n",
    "        \n",
    "        # Check and fix intersections\n",
    "        if check_any_intersection(current_polygon, reference_polygons, input_polygons, row_position):\n",
    "            intersection_count += 1\n",
    "            original_polygon = current_polygon\n",
    "            current_polygon, was_fixed = find_non_intersecting_position(\n",
    "                current_polygon,\n",
    "                reference_idx,\n",
    "                reference_polygons,\n",
    "                input_idx,\n",
    "                input_polygons,\n",
    "                row_position\n",
    "            )\n",
    "            if was_fixed:\n",
    "                fixed_count += 1\n",
    "            else:\n",
    "                unfixed_indices.append(idx)\n",
    "        \n",
    "        # Update polygon in dataframe and input_polygons list\n",
    "        gdf2.at[idx, 'geometry'] = dumps(current_polygon)\n",
    "        input_polygons[row_position] = current_polygon\n",
    "        \n",
    "        # Update input spatial index\n",
    "        input_idx.delete(row_position, input_polygons[row_position].bounds)\n",
    "        input_idx.insert(row_position, current_polygon.bounds)\n",
    "    \n",
    "    print(f\"Processed {end_idx}/{total_rows} polygons. Found {intersection_count} intersections, \"\n",
    "          f\"Fixed {fixed_count}, Unfixed {len(unfixed_indices)}, Scaled up {scaling_count}\")\n",
    "\n",
    "# Create separate dataframes for fixed and unfixed cases\n",
    "unfixed_gdf = gdf2.loc[unfixed_indices].copy()\n",
    "fixed_gdf = gdf2.drop(unfixed_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to coordinate format\n",
    "def convert_wkt_to_coordinates(wkt_str):\n",
    "    polygon = loads(wkt_str)\n",
    "    coords = list(polygon.exterior.coords)[:-1]\n",
    "    return json.dumps([{\n",
    "        'latitude': y,\n",
    "        'longitude': x\n",
    "    } for x, y in coords])\n",
    "\n",
    "print(\"Converting processed polygons back to coordinate format...\")\n",
    "fixed_gdf['Farm_polygon'] = fixed_gdf['geometry'].apply(convert_wkt_to_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final analysis\n",
    "\"\"\"\n",
    "fixed_gdf['area_acres'] = fixed_gdf['geometry'].apply(calculate_area_acres)\n",
    "fixed_gdf['is_small_polygon'] = fixed_gdf['area_acres'] < 0.125\n",
    "fixed_gdf['is_self_intersecting'] = fixed_gdf['geometry'].apply(is_self_intersecting)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(f\"\\nProcessing Results:\")\n",
    "print(\"-----------------\")\n",
    "print(f\"1. Duplicates removed: {fixed_gdf['is_duplicate'].sum()} polygons\")\n",
    "print(f\"2. Self-intersecting polygons remaining: {fixed_gdf['is_self_intersecting'].sum()}\")\n",
    "print(f\"3. Small polygons remaining: {fixed_gdf['is_small_polygon'].sum()}\")\n",
    "print(f\"4. Total intersections found: {intersection_count}\")\n",
    "print(f\"5. Successfully fixed intersections: {fixed_count}\")\n",
    "print(f\"6. Unable to fix: {len(unfixed_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "print(\"Saving results...\")\n",
    "\n",
    "# Save unfixed cases to CSV\n",
    "unfixed_gdf.to_csv('Unfixed_intersections.csv', index=True)\n",
    "print(f\"\\nUnfixed cases saved to 'unfixed_intersections.csv'\")\n",
    "\n",
    "fixed_gdf=fixed_gdf.rename(columns={'id':'Database ID','polygon_id':'Polygon ID',\"boxes_pula_id\":\"Pula Box ID\"})\n",
    "columns_to_drop = ['geometry', 'area_acres', 'is_small_polygon', 'is_self_intersecting', \n",
    "                  'is_duplicate', 'intersects_others', 'District', 'County', 'Subcounty', 'Parish']\n",
    "final_df = fixed_gdf.drop(columns=columns_to_drop, errors='ignore')\n",
    "final_df['Assignee'] = 'Xavier'\n",
    "final_df.to_csv(\"Cleaned_polygon_drawings.csv\", index=False)\n",
    "print(\"\\nProcessing complete. Results saved to: cleaned-polygon-drawings.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
